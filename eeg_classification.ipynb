{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EEG-based classification of imagined digits using a recurrent neural network  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryoii\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.fft as fft\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchvision\n",
    "import torchaudio\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import time, sys, os\n",
    "\n",
    "from utils import MindBigData, GetDataSet, GetDataLoaders, GetDataLoadersEEGImages, GetDataAndPreProcess\n",
    "from MultilayerBidirectionalLSTM import MultilayerBidirectionalLSTM\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal Acquisition & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6482, 6324, 6450, 6582, 6309, 6527, 6486, 6296, 6497, 6517]\n",
      "PreProcess - complete\n",
      "Wavelet transformation - complete\n",
      "Standardization - complete\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load EEG data from specified data path, with specified number of samples per digit.\n",
    "- GetDataSet : standardization, min-max scaling within std \n",
    "    x : 振幅 in microVolts\n",
    "    y : target digit\n",
    "- Preprocess : butterworth filter(highpass 0.1Hz + 平滑化),\n",
    " notch filter(ノイズ除去 50Hz), trim first 32 samples\n",
    "\"\"\"\n",
    "dataName = \"EP/EP1.01.txt\"\n",
    "# dataName = \"MUSE/MU.txt\"\n",
    "dataPath = os.path.join(os.path.dirname(os.getcwd()), dataName)\n",
    "\n",
    "x_raw, x_preprocessed, x_reconstructed, x_standardized, y = GetDataAndPreProcess(input_file=dataPath, samples_per_digit=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64470, 14, 256)\n",
      "(64470, 14, 256)\n",
      "(64470, 14, 256)\n",
      "(64470, 14, 256)\n",
      "(64470,)\n"
     ]
    }
   ],
   "source": [
    "# (trial, channel, data points)\n",
    "print(x_raw.shape)\n",
    "print(x_preprocessed.shape)\n",
    "print(x_reconstructed.shape)\n",
    "print(x_standardized.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train - Input shape: torch.Size([1024, 14, 256])\n",
      "train - Target shape: torch.Size([1024])\n",
      "validation - Input shape: torch.Size([1024, 14, 256])\n",
      "validation - Target shape: torch.Size([1024])\n",
      "test - Input shape: torch.Size([1024, 14, 256])\n",
      "test - Target shape: torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Prepare dataloaders 75%:15%:10%\n",
    "\"\"\"\n",
    "train_loader_raw, valid_loader_raw, test_loader_raw = GetDataLoaders(x_raw, y, batch_size=1024)\n",
    "train_loader_preprocessed, valid_loader_preprocessed, test_loader_preprocessed = GetDataLoaders(x_preprocessed, y, batch_size=1024)\n",
    "train_loader_reconstructed, valid_loader_reconstructed, test_loader_reconstructed = GetDataLoaders(x_reconstructed, y, batch_size=1024)\n",
    "train_loader_standardized, valid_loader_standardized, test_loader_standardized = GetDataLoaders(x_standardized, y, batch_size=1024)\n",
    "\n",
    "train_iter, validation_iter, test_iter = iter(train_loader_standardized), iter(valid_loader_standardized), iter(test_loader_standardized)\n",
    "\n",
    "# Check the shapes of inputs and targets\n",
    "print(\"train - Input shape:\", next(train_iter)[0].shape)\n",
    "print(\"train - Target shape:\", next(train_iter)[1].shape)\n",
    "print(\"validation - Input shape:\", next(validation_iter)[0].shape)\n",
    "print(\"validation - Target shape:\", next(validation_iter)[1].shape)\n",
    "print(\"test - Input shape:\", next(test_iter)[0].shape)\n",
    "print(\"test - Target shape:\", next(test_iter)[1].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.1004 - loss: 2.3028\n",
      "Epoch 1: val_loss improved from inf to 2.30276, saving model to best_model.keras\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - accuracy: 0.1004 - loss: 2.3029 - val_accuracy: 0.0953 - val_loss: 2.3028\n",
      "Epoch 2/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.1037 - loss: 2.3027\n",
      "Epoch 2: val_loss improved from 2.30276 to 2.30264, saving model to best_model.keras\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 2s/step - accuracy: 0.1037 - loss: 2.3027 - val_accuracy: 0.1065 - val_loss: 2.3026\n",
      "Epoch 3/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.0995 - loss: 2.3026\n",
      "Epoch 3: val_loss did not improve from 2.30264\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 2s/step - accuracy: 0.0995 - loss: 2.3026 - val_accuracy: 0.1065 - val_loss: 2.3031\n",
      "Epoch 4/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.1002 - loss: 2.3027\n",
      "Epoch 4: val_loss did not improve from 2.30264\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 2s/step - accuracy: 0.1002 - loss: 2.3027 - val_accuracy: 0.0953 - val_loss: 2.3028\n",
      "Epoch 5/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.1050 - loss: 2.3025\n",
      "Epoch 5: val_loss did not improve from 2.30264\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - accuracy: 0.1049 - loss: 2.3025 - val_accuracy: 0.1065 - val_loss: 2.3027\n",
      "Epoch 6/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.0994 - loss: 2.3027\n",
      "Epoch 6: val_loss did not improve from 2.30264\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 2s/step - accuracy: 0.0994 - loss: 2.3027 - val_accuracy: 0.0962 - val_loss: 2.3027\n",
      "Epoch 7/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.0981 - loss: 2.3026\n",
      "Epoch 7: val_loss did not improve from 2.30264\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 2s/step - accuracy: 0.0981 - loss: 2.3026 - val_accuracy: 0.0962 - val_loss: 2.3027\n",
      "Epoch 8/100\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = MultilayerBidirectionalLSTM(input_shape=(14, 256), num_classes=10)\n",
    "\n",
    "# Train the model and save the best one\n",
    "model.train(train_loader=train_loader_standardized, valid_loader=valid_loader_standardized, epochs=100, checkpoint_path='best_model.keras')\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_loader_standardized)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display 2 signals with their labels\n",
    "# data_iter = iter(train_loader_raw)\n",
    "# inputs, labels = next(data_iter)\n",
    "# EEGChannel = [\"AF3\", \"F7\", \"F3\", \"FC5\", \"T7\", \"P7\", \"O1\", \"O2\", \"P8\", \"T8\", \"FC6\", \"F4\", \"F8\", \"AF4\"]\n",
    "\n",
    "# fig = plt.figure(figsize=(20, 15))\n",
    "# for i in range(1):\n",
    "#     for j in range(14):\n",
    "#         ax = fig.add_subplot(14, 2, i + 1 + 2 * j)\n",
    "#         if (j == 0):\n",
    "#             ax.set_title(\n",
    "#                 'Raw : digit seen by patient: ' + str(labels[i].item()))  # .item() gets the value contained in a tensor\n",
    "#         ax.plot(inputs[i][j])\n",
    "#         ax.set_xlabel('samples')\n",
    "#         ax.set_ylabel(EEGChannel[j])\n",
    "#         ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display 2 signals with their labels\n",
    "# data_iter = iter(train_loader_preprocessed)\n",
    "# inputs, labels = next(data_iter)\n",
    "# EEGChannel = [\"AF3\", \"F7\", \"F3\", \"FC5\", \"T7\", \"P7\", \"O1\", \"O2\", \"P8\", \"T8\", \"FC6\", \"F4\", \"F8\", \"AF4\"]\n",
    "\n",
    "# fig = plt.figure(figsize=(20, 15))\n",
    "# for i in range(1):\n",
    "#     for j in range(14):\n",
    "#         ax = fig.add_subplot(14, 2, i + 1 + 2 * j)\n",
    "#         if (j == 0):\n",
    "#             ax.set_title(\n",
    "#                 'Preprocessed : digit seen by patient: ' + str(labels[i].item()))  # .item() gets the value contained in a tensor\n",
    "#         ax.plot(inputs[i][j])\n",
    "#         ax.set_xlabel('samples')\n",
    "#         ax.set_ylabel(EEGChannel[j])\n",
    "#         ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display 2 signals with their labels\n",
    "# data_iter = iter(train_loader_reconstructed)\n",
    "# inputs, labels = next(data_iter)\n",
    "# EEGChannel = [\"AF3\", \"F7\", \"F3\", \"FC5\", \"T7\", \"P7\", \"O1\", \"O2\", \"P8\", \"T8\", \"FC6\", \"F4\", \"F8\", \"AF4\"]\n",
    "\n",
    "# fig = plt.figure(figsize=(20, 15))\n",
    "# for i in range(1):\n",
    "#     for j in range(14):\n",
    "#         ax = fig.add_subplot(14, 2, i + 1 + 2 * j)\n",
    "#         if (j == 0):\n",
    "#             ax.set_title(\n",
    "#                 'reconstructed : digit seen by patient: ' + str(labels[i].item()))  # .item() gets the value contained in a tensor\n",
    "#         ax.plot(inputs[i][j])\n",
    "#         ax.set_xlabel('samples')\n",
    "#         ax.set_ylabel(EEGChannel[j])\n",
    "#         ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display 2 signals with their labels\n",
    "# data_iter = iter(train_loader_standardized)\n",
    "# inputs, labels = next(data_iter)\n",
    "# EEGChannel = [\"AF3\", \"F7\", \"F3\", \"FC5\", \"T7\", \"P7\", \"O1\", \"O2\", \"P8\", \"T8\", \"FC6\", \"F4\", \"F8\", \"AF4\"]\n",
    "\n",
    "# fig = plt.figure(figsize=(20, 15))\n",
    "# for i in range(1):\n",
    "#     for j in range(14):\n",
    "#         ax = fig.add_subplot(14, 2, i + 1 + 2 * j)\n",
    "#         if (j == 0):\n",
    "#             ax.set_title(\n",
    "#                 'standardized : digit seen by patient: ' + str(labels[i].item()))  # .item() gets the value contained in a tensor\n",
    "#         ax.plot(inputs[i][j])\n",
    "#         ax.set_xlabel('samples')\n",
    "#         ax.set_ylabel(EEGChannel[j])\n",
    "#         ax.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
